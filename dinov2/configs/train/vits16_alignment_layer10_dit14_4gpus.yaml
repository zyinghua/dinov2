train:
  batch_size_per_gpu: 10
student:
  arch: vit_small
  patch_size: 16
  drop_path_rate: 0
optim:
  epochs: 120
evaluation:
  eval_period_iterations: 6250

# Alignment configuration for DINOv2-DiT alignment
# Trial 1: DINOv2 layer 10, DiT layer 14, fixed timestep, full alignment (no curriculum learning)
alignment:
  enabled: true  # Set to false to disable alignment
  
  # DINOv2 alignment settings
  alignment_depth: 10  # Which DINOv2 layer to extract features from (0-indexed, -1 = no alignment)
  alignment_loss_weight: 0.1  # Weight for alignment loss in total loss
  
  # DiT model settings
  dit_model_path: "/root/autodl-tmp/pretrained_models/DiT-XL-2-256x256.pt"  # Path to pretrained DiT checkpoint
  dit_model_name: "DiT-XL/2"  # DiT architecture name (e.g., "DiT-XL/2", "DiT-L/2", "DiT-B/2")
  dit_extraction_layer: 14  # Which DiT layer to extract features from (-1 = last layer, 0-indexed)
  dit_timestep: 350.0  # Fixed timestep for DiT forward pass (0-999, 350 = moderate noise level, good balance for noisy training data)
  
  # Projector settings
  dit_hidden_dim: 1152  # DiT hidden dimension, 1152 for XL
  projector_dim: 2048  # MLP projector hidden dimension (same as REPA)
  
  # Loss function settings
  alignment_loss_type: "cosine_sim"  # Loss type: "cosine_sim", "mse", or "l1"

