{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355UKMUQJxFd"
   },
   "source": [
    "# Scalable Diffusion Models with Transformer (DiT)\n",
    "\n",
    "This notebook samples from pre-trained DiT models. DiTs are class-conditional latent diffusion models trained on ImageNet that use transformers in place of U-Nets as the DDPM backbone. DiT outperforms all prior diffusion models on the ImageNet benchmarks.\n",
    "\n",
    "[Project Page](https://www.wpeebles.com/DiT) | [HuggingFace Space](https://huggingface.co/spaces/wpeebles/DiT) | [Paper](http://arxiv.org/abs/2212.09748) | [GitHub](github.com/facebookresearch/DiT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJlgLkSaKn7u"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU). Run this cell to clone the DiT GitHub repo and setup PyTorch. You only have to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda-env/dit/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/facebookresearch/DiT.git\n",
    "import os\n",
    "\n",
    "# Change to DiT directory if it exists (handle both cases: already in DiT or need to navigate to it)\n",
    "if os.path.exists('DiT') and os.path.isdir('DiT'):\n",
    "    os.chdir('DiT')\n",
    "elif os.path.basename(os.getcwd()) != 'DiT':\n",
    "    # If we're not in DiT and DiT doesn't exist as subdirectory, check if we need to clone it\n",
    "    print(\"Warning: DiT directory not found. Make sure you've cloned the repository or are already in the DiT directory.\")\n",
    "\n",
    "# Set PYTHONPATH to include DiT directory\n",
    "dit_path = os.getcwd() if os.path.basename(os.getcwd()) == 'DiT' else os.path.join(os.getcwd(), 'DiT')\n",
    "os.environ['PYTHONPATH'] = os.pathsep.join([os.environ.get('PYTHONPATH', ''), dit_path]).strip(os.pathsep)\n",
    "\n",
    "# !pip install diffusers timm --upgrade\n",
    "# DiT imports:\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model\n",
    "from models import DiT_XL_2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXpziRkoOvV9"
   },
   "source": [
    "# Download DiT-XL/2 Models\n",
    "\n",
    "You can choose between a 512x512 model and a 256x256 model. You can swap-out the LDM VAE, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EWG-WNimO59K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda-env/dit/lib/python3.14/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-mse\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "# Load model:\n",
    "model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval() # important!\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Encoding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image tensor:\n",
      "  Shape: torch.Size([1, 3, 256, 256])\n",
      "\n",
      "Encoded latent tensor:\n",
      "  Shape: torch.Size([1, 4, 32, 32])\n",
      "\n",
      "Verifying round-trip (encode -> decode):\n",
      "  Decoded image shape: torch.Size([1, 3, 256, 256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_image = torch.randn(1, 3, 256, 256, device=device)\n",
    "\n",
    "print(\"Original image tensor:\")\n",
    "print(f\"  Shape: {fake_image.shape}\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_dist = vae.encode(fake_image)\n",
    "    latents = latent_dist.latent_dist.sample()\n",
    "    \n",
    "    latents = latents * 0.18215\n",
    "\n",
    "print(\"Encoded latent tensor:\")\n",
    "print(f\"  Shape: {latents.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Verifying round-trip (encode -> decode):\")\n",
    "with torch.no_grad():\n",
    "    decoded_latents = latents / 0.18215\n",
    "    decoded_image = vae.decode(decoded_latents).sample\n",
    "\n",
    "print(f\"  Decoded image shape: {decoded_image.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JTNyzNZKb9E"
   },
   "source": [
    "# 2. Sample from Pre-trained DiT Models\n",
    "\n",
    "You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Hw7B5h4Kk4p"
   },
   "outputs": [],
   "source": [
    "# Set user inputs:\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = 207, 360, 387, 974, 88, 979, 417, 279 #@param {type:\"raw\"}\n",
    "samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "# Create diffusion object:\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# Create sampling noise:\n",
    "n = len(class_labels)\n",
    "z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# Setup classifier-free guidance:\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.tensor([1000] * n, device=device)\n",
    "y = torch.cat([y, y_null], 0)\n",
    "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# Sample images:\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "    model_kwargs=model_kwargs, progress=True, device=device\n",
    ")\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "samples = vae.decode(samples / 0.18215).sample\n",
    "print(samples.shape)\n",
    "# Save and display images:\n",
    "save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "           normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_intermediate_states(intermediate_states):\n",
    "    \"\"\"\n",
    "    Print information about captured intermediate states from DiT blocks.\n",
    "    \"\"\"\n",
    "    if intermediate_states is None:\n",
    "        print(\"No intermediate states captured. Run forward with capture_intermediates=True\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== DiT Intermediate States ===\")\n",
    "    print(f\"Number of blocks: {len(intermediate_states)}\")\n",
    "    print()\n",
    "\n",
    "    for i, state in enumerate(intermediate_states):\n",
    "        print(f\"Block {i+1}/{len(intermediate_states)}:\")\n",
    "        print(f\"  Shape: {state.shape}\")\n",
    "        print(f\"  Dtype: {state.dtype}\")\n",
    "        print(f\"  Device: {state.device}\")\n",
    "        print(f\"  Mean: {state.mean().item():.6f}\")\n",
    "        print(f\"  Std: {state.std().item():.6f}\")\n",
    "        print(f\"  Min: {state.min().item():.6f}\")\n",
    "        print(f\"  Max: {state.max().item():.6f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intermediate state capture...\n",
      "Input shape: torch.Size([1, 4, 32, 32])\n",
      "Timestep: 50\n",
      "Class label: 207\n",
      "\n",
      "Number of DiT blocks: 28\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 8, 32, 32])\n",
      "\n",
      "\n",
      "=== DiT Intermediate States ===\n",
      "Number of blocks: 28\n",
      "\n",
      "Block 1/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.340069\n",
      "  Std: 6.217363\n",
      "  Min: -207.843369\n",
      "  Max: 27.979557\n",
      "\n",
      "Block 2/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.350029\n",
      "  Std: 7.762352\n",
      "  Min: -226.137863\n",
      "  Max: 213.592316\n",
      "\n",
      "Block 3/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.398417\n",
      "  Std: 11.373156\n",
      "  Min: -344.295044\n",
      "  Max: 230.000381\n",
      "\n",
      "Block 4/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.183600\n",
      "  Std: 18.989187\n",
      "  Min: -611.892395\n",
      "  Max: 230.037811\n",
      "\n",
      "Block 5/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.112269\n",
      "  Std: 22.350737\n",
      "  Min: -786.311951\n",
      "  Max: 230.079391\n",
      "\n",
      "Block 6/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.040154\n",
      "  Std: 25.030758\n",
      "  Min: -828.018738\n",
      "  Max: 229.993027\n",
      "\n",
      "Block 7/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.054013\n",
      "  Std: 25.291895\n",
      "  Min: -939.826904\n",
      "  Max: 229.565689\n",
      "\n",
      "Block 8/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.074763\n",
      "  Std: 25.506498\n",
      "  Min: -914.924866\n",
      "  Max: 229.579773\n",
      "\n",
      "Block 9/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.094724\n",
      "  Std: 24.682032\n",
      "  Min: -891.448608\n",
      "  Max: 229.442352\n",
      "\n",
      "Block 10/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 0.107787\n",
      "  Std: 25.209913\n",
      "  Min: -925.266479\n",
      "  Max: 229.413300\n",
      "\n",
      "Block 11/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.047149\n",
      "  Std: 28.300812\n",
      "  Min: -1091.972412\n",
      "  Max: 229.420242\n",
      "\n",
      "Block 12/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.123547\n",
      "  Std: 36.236935\n",
      "  Min: -1435.546387\n",
      "  Max: 229.457947\n",
      "\n",
      "Block 13/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.482270\n",
      "  Std: 45.905586\n",
      "  Min: -1743.224121\n",
      "  Max: 229.687408\n",
      "\n",
      "Block 14/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.648329\n",
      "  Std: 45.805595\n",
      "  Min: -1915.384399\n",
      "  Max: 229.632309\n",
      "\n",
      "Block 15/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.707510\n",
      "  Std: 52.589348\n",
      "  Min: -2248.266357\n",
      "  Max: 273.602570\n",
      "\n",
      "Block 16/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -1.600279\n",
      "  Std: 79.291176\n",
      "  Min: -3200.827881\n",
      "  Max: 247.691406\n",
      "\n",
      "Block 17/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -1.513366\n",
      "  Std: 76.569374\n",
      "  Min: -3163.222656\n",
      "  Max: 326.087067\n",
      "\n",
      "Block 18/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -1.387241\n",
      "  Std: 83.220718\n",
      "  Min: -3333.387451\n",
      "  Max: 276.248596\n",
      "\n",
      "Block 19/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -3.483566\n",
      "  Std: 141.155411\n",
      "  Min: -3951.099121\n",
      "  Max: 364.414886\n",
      "\n",
      "Block 20/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -3.471581\n",
      "  Std: 142.599380\n",
      "  Min: -4201.280762\n",
      "  Max: 526.207703\n",
      "\n",
      "Block 21/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -3.778613\n",
      "  Std: 160.117615\n",
      "  Min: -5021.507812\n",
      "  Max: 529.453918\n",
      "\n",
      "Block 22/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -3.217687\n",
      "  Std: 173.848785\n",
      "  Min: -6248.127930\n",
      "  Max: 570.330933\n",
      "\n",
      "Block 23/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -2.823037\n",
      "  Std: 231.175812\n",
      "  Min: -8024.654297\n",
      "  Max: 413.618591\n",
      "\n",
      "Block 24/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -2.569382\n",
      "  Std: 258.083282\n",
      "  Min: -9024.097656\n",
      "  Max: 419.995789\n",
      "\n",
      "Block 25/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -2.559534\n",
      "  Std: 294.571014\n",
      "  Min: -12092.614258\n",
      "  Max: 520.330444\n",
      "\n",
      "Block 26/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: -0.908051\n",
      "  Std: 322.598572\n",
      "  Min: -11635.400391\n",
      "  Max: 619.343140\n",
      "\n",
      "Block 27/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 1.990562\n",
      "  Std: 251.545242\n",
      "  Min: -9269.628906\n",
      "  Max: 810.990784\n",
      "\n",
      "Block 28/28:\n",
      "  Shape: torch.Size([1, 256, 1152])\n",
      "  Dtype: torch.float32\n",
      "  Device: cuda:0\n",
      "  Mean: 3.295567\n",
      "  Std: 201.251480\n",
      "  Min: -4774.182129\n",
      "  Max: 2942.304199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test: Capture and print intermediate states of DiT blocks\n",
    "\n",
    "# Create a test input\n",
    "test_z = torch.randn(1, 4, latent_size, latent_size, device=device)\n",
    "test_t = torch.tensor([50], device=device)  # timestep\n",
    "test_y = torch.tensor([207], device=device)  # class label\n",
    "\n",
    "print(\"Testing intermediate state capture...\")\n",
    "print(f\"Input shape: {test_z.shape}\")\n",
    "print(f\"Timestep: {test_t.item()}\")\n",
    "print(f\"Class label: {test_y.item()}\")\n",
    "print(f\"\\nNumber of DiT blocks: {len(model.blocks)}\")\n",
    "print()\n",
    "\n",
    "# Run forward pass with intermediate state capture\n",
    "with torch.no_grad():\n",
    "    output, intermediate_states = model.forward(test_z, test_t, test_y, capture_intermediates=True)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# Print intermediate states\n",
    "print_intermediate_states(intermediate_states)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
